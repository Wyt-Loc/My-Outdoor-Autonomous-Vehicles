# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
"""
Run YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python detect.py --weights yolov5s.pt --source 0                               # webcam
                                                     img.jpg                         # image
                                                     vid.mp4                         # video
                                                     screen                          # screenshot
                                                     path/                           # directory
                                                     list.txt                        # list of images
                                                     list.streams                    # list of streams
                                                     'path/*.jpg'                    # glob
                                                     'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle
"""

import argparse
import multiprocessing
import os
import platform
import sys
import threading
import time
from pathlib import Path
import torch

import ç½‘ç»œè¿œç¨‹æ§åˆ¶éƒ¨åˆ†.ç‰ˆæœ¬3.Modules.Control.MyControl
from models.common import DetectMultiBackend
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams
from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,
                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import select_device, smart_inference_mode
import ç½‘ç»œè¿œç¨‹æ§åˆ¶éƒ¨åˆ†.ç‰ˆæœ¬3.Modules.Control.MyLidar as Lidar
import heapq
import socket
import numpy as np
import matplotlib

matplotlib.use("Qt5Agg")
import matplotlib.pyplot as plt
import time
import threading
import serial.tools.list_ports

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

# è®¾å¤‡å½“å‰ç›¸å¯¹è§’åº¦
xcar_jiao = 0
ycar_jiao = 0
zcar_jiao = 0
# è®¾å¤‡å½“å‰ç›¸å¯¹ä½ç½®
xcar = 0
ycar = 0
zcar = 0
# åœ°å›¾
xyz = True  # é”ï¼šå½“å¯¹xyzä¿®æ”¹çš„æ—¶å€™ä¸å…è®¸ä½¿ç”¨
x = []  # å³æ­£
y = []  # å‰æ­£
z = []  # ä¸Šæ­£
# æ€»å¼€å…³
OK = True

# å®šä¹‰å…¨å±€å˜é‡ æ²¡åŠæ³•äº†ï¼Œåªèƒ½è¿™æ ·å–å€¼äº†
# å¾—åˆ°æ ‡ç­¾å€¼
gettxt = []
px = 0
py = 0

lableIngo = multiprocessing.Queue(100)


# å§¿æ€
class myThread_zitai(threading.Thread):
    def __init__(self, name, showok):
        threading.Thread.__init__(self)
        self.name = name
        self.showok = showok
        self.use = True
        self.LEN = 28
        self.LENJIA = 12
        self.LENJIAO = 12
        self.jiuzhou_data = []  # åŠ é€Ÿåº¦ï¼Œå®æ—¶è®¡ç®—æ‰€åœ¨ä½ç½® # è§’é€Ÿåº¦ï¼Œç§¯åˆ†å¾—åˆ°å½“å‰è§’åº¦
        self.lasetime = [time.time()]  # ä¸ºäº†å¯¹é€Ÿåº¦å’Œè§’é€Ÿåº¦è¿›è¡Œç§¯åˆ†
        self.js = 0
        self.lasetime2 = time.time()  # ä¸ºäº†é…åˆjså®ç°è¾“å‡ºé€Ÿåº¦æ•ˆæœ
        self.lastxv = 0
        self.lastyv = 0
        self.lastzv = 0
        self.lastxj = 0
        self.lastyj = 0
        self.lastzj = 0
        self.ser = None

    def print(self, *string):
        if self.showok:
            print(self.name + " - - ", end="")
            for i in string:
                print(i, end=' ')
            print()

    def end(self):
        self.use = False

    def updata_weizhi(self):
        global xcar, ycar, zcar, xcar_jiao, ycar_jiao, zcar_jiao
        if self.jiuzhou_data:  # éç©º
            if len(self.jiuzhou_data) == len(self.lasetime) - 1:  # ä¸”ç­‰å¤§ï¼Œå¦åˆ™å°±æ˜¯æœ‰é—®é¢˜
                for i in range(0, len(self.jiuzhou_data)):
                    oncetime = self.lasetime[i + 1] - self.lasetime[i]  # è®¡ç®—æ—¶é—´é—´éš”
                    """ä»¥ä¸‹ä½ç½®ã€æ–¹å‘ä¿¡æ¯ç›´æ¥å†™åˆ°å…¨å±€å˜é‡ä¸­ï¼Œæ–¹ä¾¿ä½¿ç”¨"""
                    # print(i,len(self.jiuzhou_data),len(self.jiuzhou_data[0]))  # æµ‹è¯•æ•°æ®
                    xc = xcar_jiao + oncetime * (self.jiuzhou_data[i][3] + self.lastxj) / 2  # è®¡ç®—è§’åº¦å€¼
                    yc = ycar_jiao + oncetime * (self.jiuzhou_data[i][4] + self.lastyj) / 2
                    zc = zcar_jiao + oncetime * (self.jiuzhou_data[i][5] + self.lastzj) / 2
                    # print(self.jiuzhou_data[i][5]-self.lastzj)
                    if self.jiuzhou_data[i][3] - self.lastxj < 300 and self.jiuzhou_data[i][4] - self.lastyj < 300 and \
                            self.jiuzhou_data[i][5] - self.lastzj < 300:  # é˜²æ­¢çªå˜ï¼Œå…¶å®è¿˜æ˜¯æ•°æ®éªŒè¯æœ‰é—®é¢˜
                        xcar_jiao = xc + 0.00011  # åƒè¾›ä¸‡è‹¦æ ‡å®šå‡ºæ¥çš„æ•°æ®ï¼ŒæŠµæŠ—å›ºæœ‰åç§»
                        ycar_jiao = yc + 0.00078  # é™æ€ç²¾åº¦æ¯äºŒåˆ†ä¹‹ä¸€cmdä¸è¶…è¿‡0.1Â°çš„åç§»
                        zcar_jiao = zc - 0.000412  # ä¹Ÿå¯é‡‡ç”¨ç¨‹åºæ ‡å®šï¼Œé™æ­¢é˜²æ­¢ä¸€å®šæ—¶é—´ï¼Œç»ˆå€¼é™¤ä»¥æ¬¡æ•°ï¼Œä»¥åè¿™ä¸ªæ•°å€¼ä¸å‡†ç¡®äº†å†ç”¨
                        self.lastxj = self.jiuzhou_data[i][3]
                        self.lastyj = self.jiuzhou_data[i][4]
                        self.lastzj = self.jiuzhou_data[i][5]
                        # self.print("å½“å‰æ–¹å‘è§’ä¸º: ", zcar_jiao)
                        """é€šè¿‡uwbå®šä½ç³»ç»Ÿè¿›è¡Œå®šä½ï¼Œæš‚æ—¶ä¸ä½¿ç”¨æƒ¯å¯¼ï¼Œæ—¶é—´é•¿äº†éš¾å…æ¼‚ç§»ï¼Œ
                        å¦å¤–ï¼Œè¿˜éœ€è¦åœ¨ä»¥ä¸‹ä»£ç åŸºç¡€ä¸Šè€ƒè™‘å¦‚ä½•å»æ‰é‡åŠ›åŠ é€Ÿåº¦ï¼Œç°å®çŠ¶å†µä¸‹zè½´ä¸å¯èƒ½æ—¶åˆ»ç«–ç›´ï¼Œ
                        æ‰€ä»¥xyzæ–¹å‘ä¸Šéƒ½æœ‰åˆ†é‡ï¼Œå…·ä½“è¯¥å¦‚ä½•å»é™¤ï¼Œç°åœ¨æœ‰ç‚¹æƒ³æ³•ï¼Œæ˜¯åˆ©ç”¨å®é™…çº¿é€Ÿåº¦ä¸º0æ—¶å€™ï¼Œ
                        è®°å½•æ­¤æ—¶çš„å„ä¸ªåŠ é€Ÿåº¦å€¼ï¼Œæ¯æ¬¡è®¡ç®—è¿›è¡Œæ‰£é™¤ï¼Œæƒ³æ³•ç•™å¾…ä»¥åå®éªŒ
                        """
                        """
                        xcar = xcar + self.lastxv * oncetime + self.jiuzhou_data[i][0] * oncetime * oncetime / 2
                        ycar = ycar + self.lastyv * oncetime + self.jiuzhou_data[i][1] * oncetime * oncetime / 2
                        zcar = zcar + self.lastzv * oncetime + self.jiuzhou_data[i][2] * oncetime * oncetime / 2
                        self.lastxv = self.lastxv + self.jiuzhou_data[i][0] * oncetime
                        self.lastyv = self.lastyv + self.jiuzhou_data[i][1] * oncetime
                        self.lastzv = self.lastzv + self.jiuzhou_data[i][2] * oncetime
                        self.print(xcar, "   ", ycar, "   ", zcar)
                        """
                """å°†å¤„ç†å®Œæ¯•çš„æ•°æ®è¿›è¡Œæˆªé™¤"""
                self.jiuzhou_data = []  # å¤„ç†å®Œäº†ï¼Œæ¸…ç©º
                self.lasetime = self.lasetime[-1:]  # åªå‰©ä¸‹æœ€åä¸€é¡¹ä½œä¸ºä¸€ä¸‹æ¬¡çš„é¦–ä½æ—¶é—´
            else:
                self.print("æ•°æ®é•¿åº¦é”™è¯¯ï¼Œä¸è¿›è¡Œè§£å†³å°†ä¼šäº§ç”Ÿä¸¥é‡é”™è¯¯ï¼Œå°†è‡ªè¡Œé€€å‡ºå§¿æ€çº¿ç¨‹")
                self.end()

    def sudu(self):
        """æ¯æ¬¡æˆåŠŸå­˜ä¸€å¸§æ•°æ®ä¹‹åï¼Œå¿…é¡»å­˜æ­¤æ—¶çš„æ—¶é—´æˆ³"""
        self.lasetime.append(time.time())
        """è¿›è¡Œè¾“å‡ºæ˜¾ç¤º"""
        self.js = self.js + 1
        if self.js == 100:
            self.print((self.lasetime[-1] - self.lasetime2) / 100, "s/å¸§")
            self.lasetime2 = self.lasetime[-1]
            self.js = 0

    def jisuan(self, a, b, c, d):
        """è´Ÿæ•°ç”¨è¡¥ç ï¼ˆå¤§å‘å•Šï¼‰"""
        byte16 = (a << 0) | (b << 8) | (c << 16) | (d << 24)
        if ((byte16 & 0x80000000) == 0):
            result = byte16
        else:
            byte16 = byte16 ^ 0xffffffff
            byte16 = byte16 + 1
            result = -byte16
        return result * 0.000001

    def yanzheng(self, data, id0, id1):
        """æ±‚å’ŒéªŒè¯æ•°æ®ä¼ è¾“å‡†ç¡®æ€§
            ä¸‹è¿°æ–¹æ³•æ˜¯ç»éªŒä¹‹è°ˆï¼Œå’Œæ–‡æ¡£æœ‰å‡ºå…¥ï¼Œ
            å¯ä»¥çœ‹åˆ°éªŒè¯æ•°æ®æ—¶å€™æˆ‘åªä½¿ç”¨ä½8ä½è¿›è¡Œæ£€éªŒï¼Œå®å±æ— å¥ˆä¹‹ä¸¾ï¼Œ
            ä¸Šé¢çš„å‡ ä½æ€»æ˜¯ä¸ä¸€è‡´ï¼Œé€šè¿‡ä¸äº†éªŒè¯ï¼Œæ‰€ä»¥æˆ‘æ‰æ”¾ä½äº†æ¡ä»¶ï¼Œäº‹å®è¯´æ˜ç”¨èµ·æ¥è¿˜å¯ä»¥
        """
        CK1 = 0
        CK2 = 0
        for i in data[id0:id1 + 1]:
            CK1 = CK1 + i
            CK2 = CK2 + CK1
        CK = (CK2 << 8 | CK1) & 0x0000ff  # å†—ä½™ä»£ç 
        CKc = (data[id1 + 2] << 8 | data[id1 + 1]) & 0x0000ff  # å†—ä½™ä»£ç 
        if CK == CKc:
            return True
        else:
            # print(bin(CK))  # ç”¨äºæµ‹è¯•
            # print(bin(CKc))
            # print('bbbbb:', data[id0-2:id1+5])
            return False

    def jiexi(self, data):
        for i, ii in enumerate(data):
            if ii == 89 and data[i + 1] == 83:  # æ ‡å¿—ä½æ£€éªŒ
                if data[i + 4] == self.LEN:  # éªŒè¯æ•°æ®å¯é æ€§
                    if data[i + 5] == 0x10 and data[i + 6] == self.LENJIA and data[i + 19] == 0x20 and data[
                        i + 20] == self.LENJIAO:  # åé¢çš„12å­—èŠ‚æ˜¯åŠ é€Ÿåº¦å€¼ã€åé¢çš„12å­—èŠ‚æ˜¯è§’é€Ÿåº¦å€¼
                        if self.yanzheng(data, i + 2, i + 32):
                            xa = self.jisuan(data[i + 7], data[i + 8], data[i + 9], data[i + 10])  # xåŠ é€Ÿåº¦
                            ya = self.jisuan(data[i + 11], data[i + 12], data[i + 13], data[i + 14])  # yåŠ é€Ÿåº¦
                            za = self.jisuan(data[i + 15], data[i + 16], data[i + 17], data[i + 18])  # zåŠ é€Ÿåº¦
                            xwa = self.jisuan(data[i + 21], data[i + 22], data[i + 23], data[i + 24])  # xè§’åŠ é€Ÿåº¦
                            ywa = self.jisuan(data[i + 25], data[i + 26], data[i + 27], data[i + 28])  # yè§’åŠ é€Ÿåº¦
                            zwa = self.jisuan(data[i + 29], data[i + 30], data[i + 31], data[i + 32])  # zè§’åŠ é€Ÿåº¦
                            self.jiuzhou_data.append([xa, ya, za, xwa, ywa, zwa])
                            self.sudu()  # æ¯æ‰§è¡Œä¸€ä¸ªæ•°æ®è§£æåˆ™è¿›è¡Œä¸€æ¬¡æ—¶é—´é‡‡é›†
                            # self.print(onezhen)
                            return self.jiexi(data[i + 35:])  # CK1ã€CK2ä¹Ÿç®—ä¸Š  # é€’å½’æŸ¥æ‰¾ä¸‹ä¸€å¸§ï¼Œç›´åˆ°return data
        return data

    def run(self):
        self.print("çº¿ç¨‹å¯åŠ¨")
        try:
            self.ser = serial.Serial("COM4", 921600)  # ä½ç½®1
            self.ser.flushInput()  # ä½ç½®2
            self.print("ä¸²å£åˆå§‹åŒ–å®Œæˆ")
        except:
            self.print("ä¸²å£åˆå§‹åŒ–å¤±è´¥!!!")
            self.end()
        recv = []
        while self.use and OK:
            count = self.ser.inWaiting()
            if count != 0:
                recv = recv + list(self.ser.read(count))  # listè¿æ¥
                self.ser.flushInput()  # æ¸…ç©ºç¼“å†²åŒº
                if len(recv) > 200:
                    try:
                        # print("rece = ", recv)
                        recv = self.jiexi(recv)  # è§£ææ•°æ®
                        self.updata_weizhi()  # å°†æ‰€æœ‰å¾—åˆ°çš„æ•°æ®è¿›è¡Œè®¡ç®—ï¼Œå¾—åˆ°ä½ç½®ä¿¡æ¯åˆ°å…¨å±€å˜é‡ä¸­
                    except BaseException as e:
                        pass
                        # self.print("è§£æé”™è¯¯:", e)
        self.print("çº¿ç¨‹é€€å‡º")


# åœ°å€ 192.168.0.100
# ç«¯å£ 8487


class Lidar(ç½‘ç»œè¿œç¨‹æ§åˆ¶éƒ¨åˆ†.ç‰ˆæœ¬3.Modules.Control.MyControl.Mykey):
    lidarData = []
    lidarmap = []
    lidarDistance = []
    lidarAngle = []  # åŸå§‹è§’åº¦
    lidarAngle_old = []
    lidarDis_old = []
    x1 = []
    y1 = []
    path = []
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    def __init__(self):
        print("æ­£åœ¨è¿æ¥é›·è¾¾.....")
        # è¿æ¥æ¿€å…‰é›·è¾¾
        RECV_BUF_SIZE = 4096
        self.s.setsockopt(  # åŠ å…¥ æ­¤é…ç½® è§£å†³TCPæ•°æ®å»¶è¿Ÿé—®é¢˜ï¼Œåœ¨å› ä¸ºé»˜è®¤ç¼“å†²åŒºå¤§å°ç»´65535 é‚£ä¹ˆç¬¬ä¸€æ¬¡å¯èƒ½æ˜¯è¦æ¥æ»¡
            # æ‰ä¼šæœ‰ä¸‹ä¸€æ¬¡ï¼Œæ‰€ä»¥ä¹‹åæ•°æ®ä¼šä¸€ç›´å»¶è¿Ÿ5ç§’ï¼ŒåŠ å…¥æ­¤é¡¹è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
            socket.SOL_SOCKET,
            socket.SO_RCVBUF,
            RECV_BUF_SIZE)
        self.s.connect_ex(('192.168.0.100', 8487))
        print("æ¿€å…‰é›·è¾¾è¿æ¥æˆåŠŸ")

    def receLidarData(self):
        # æ¥æ”¶TCPæ•°æ®
        self.lidarData = self.s.recv(4096)

    def getDistance(self, data1: int, data2: int, data3: int) -> int:
        # å¾—åˆ°è·ç¦»
        return ((data1 & 0x1) << 14) + ((data2 & 0x7F) << 7) + (data3 & 0x7F)

    # cbit ä¸ºå¯¹åº” 0~255 çš„ 1 çš„ä¸ªæ•°è¡¨

    def getCrcPackage4Byte(self, a, b, c, d) -> bool:
        cbit = [
            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
            1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
            1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
            2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
            1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
            2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
            2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
            3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 4, 5, 5, 6, 5, 6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8]
        numone = (cbit[b] + cbit[c] + cbit[d]) & 0x07  # å¾—åˆ° 1 çš„ä¸ªæ•°
        if numone == ((a & 0x70) >> 4):
            return True
        else:
            return False

    def heuristic(self, a, b):
        # è®¡ç®—å¯å‘å¼å‡½æ•°ï¼ˆæ›¼å“ˆé¡¿è·ç¦»ï¼‰
        return abs(b[0] - a[0]) + abs(b[1] - a[1])

    def astar(self, start, goal):
        # å®šä¹‰å¯è¡ŒåŠ¨çš„æ–¹å‘ï¼ˆä¸Šã€ä¸‹ã€å·¦ã€å³å’Œå¯¹è§’çº¿ï¼‰
        directions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]
        # åˆå§‹åŒ–èµ·ç‚¹å’Œç»ˆç‚¹
        start_node = (self.heuristic(start, goal), start, 0, None)
        goal_node = (self.heuristic(start, goal), goal, float('inf'), None)
        # å®šä¹‰å¼€æ”¾åˆ—è¡¨å’Œå…³é—­åˆ—è¡¨
        open_list = []
        closed_list = set()
        # å°†èµ·ç‚¹åŠ å…¥å¼€æ”¾åˆ—è¡¨
        heapq.heappush(open_list, start_node)
        while open_list:
            # ä»å¼€æ”¾åˆ—è¡¨ä¸­å–å‡ºå…·æœ‰æœ€å°é¢„ä¼°æ€»ä»£ä»·çš„èŠ‚ç‚¹
            current_node = heapq.heappop(open_list)
            current_pos = current_node[1]
            # åˆ°è¾¾ç›®æ ‡èŠ‚ç‚¹
            if current_pos == goal_node[1]:
                path = []
                # ä»ç»ˆç‚¹å›æº¯è‡³èµ·ç‚¹å¾—åˆ°è·¯å¾„
                while current_node:
                    path.append(current_node[1])
                    current_node = current_node[3]
                return list(reversed(path))
            # å°†å½“å‰èŠ‚ç‚¹æ ‡è®°ä¸ºå…³é—­çŠ¶æ€
            closed_list.add(current_pos)
            try:
                # éå†å¯è¡ŒåŠ¨çš„æ–¹å‘
                for direction in directions:
                    next_pos = (current_pos[0] + direction[0], current_pos[1] + direction[1])
                    # æ£€æŸ¥ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ˜¯å¦è¶Šç•Œæˆ–ä¸ºéšœç¢ç‰©
                    if next_pos[0] < 0 or next_pos[0] >= len(self.lidarmap) or next_pos[1] < 0 or \
                            next_pos[1] >= len(self.lidarmap[0]) or \
                            self.lidarmap[next_pos[0]][next_pos[1]] == 1:
                        continue
                    # è®¡ç®—ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„ä»£ä»·å’Œé¢„ä¼°æ€»ä»£ä»·
                    cost = current_node[2] + 1
                    next_node = (cost + self.heuristic(next_pos, goal), next_pos, cost, current_node)
                    # å¦‚æœä¸‹ä¸€ä¸ªèŠ‚ç‚¹å·²ç»åœ¨å…³é—­åˆ—è¡¨ä¸­ï¼Œåˆ™è·³è¿‡
                    if next_pos in closed_list:
                        continue
                    # å¦‚æœä¸‹ä¸€ä¸ªèŠ‚ç‚¹å·²ç»åœ¨å¼€æ”¾åˆ—è¡¨ä¸­ä¸”ä»£ä»·æ›´é«˜ï¼Œåˆ™è·³è¿‡
                    for node in open_list:
                        if next_pos == node[1] and cost >= node[2]:
                            break
                    else:
                        # å°†ä¸‹ä¸€ä¸ªèŠ‚ç‚¹åŠ å…¥å¼€æ”¾åˆ—è¡¨
                        heapq.heappush(open_list, next_node)
            except:
                pass
        # å¦‚æœå¼€æ”¾åˆ—è¡¨ä¸ºç©ºï¼Œè¡¨ç¤ºæ— æ³•åˆ°è¾¾ç›®æ ‡èŠ‚ç‚¹
        return []

    # è½¬åˆ°ç¬›å¡å°”åæ ‡ç³»
    def polar_to_cartesian(self, r, theta):
        x = r * np.cos(theta)
        y = r * np.sin(theta)
        return x, y

    def parsingLidarData(self):
        self.lidarDistance = []
        self.lidarAngle = []  # åŸå§‹è§’åº¦
        self.receLidarData()
        for i, val in enumerate(self.lidarData):
            # å¯¹åº”4ä¸ªå­—èŠ‚ é˜²æ­¢è¶Šç•Œ
            if i <= (len(self.lidarData) - 4) \
                    and self.lidarData[i] & 0x80 == 0 \
                    and self.lidarData[i + 1] & 0x80 == 0 \
                    and self.lidarData[i + 2] & 0x80 == 0 \
                    and self.lidarData[i + 3] & 0x80 == 0x80:
                # æ ¡éªŒ BCD
                if self.getCrcPackage4Byte(self.lidarData[i], self.lidarData[i + 1], self.lidarData[i + 2],
                                           self.lidarData[i + 3]):
                    distancef = (self.lidarData[i] & 0x0F)
                    distancef <<= 7
                    distancef += self.lidarData[i + 1] & 0x7F
                    distancef <<= 1
                    self.lidarDistance.append(distancef)
                    self.lidarDis_old.append(distancef)
                    if self.lidarData[i + 2] & 0x40:
                        self.lidarDistance[0] += 1
                    anglef = (self.lidarData[i + 2] & 0x3F)
                    anglef <<= 7
                    anglef += self.lidarData[i + 3] & 0x7F
                    anglef = anglef / 16
                    self.lidarAngle.append(anglef)
                    self.lidarAngle_old.append(anglef)
                    # print("æ–¹å‘ = ", anglef, "è·ç¦» = ", distancef)

    def generationMap(self):
        # ç”Ÿæˆåœ°å›¾
        for i in range(len(self.lidarAngle)):
            self.lidarAngle[i] = self.lidarAngle[i] * (np.pi / 180)  # æåæ ‡
        self.x1, self.y1 = self.polar_to_cartesian(self.lidarDistance, self.lidarAngle)
        # for i in range(len(self.x1)):
        #     self.x1[i] = self.x1[i]
        #     self.y1[i] = self.y1[i]
        # xmax = int(max(self.x1) + 1)
        # ymax = int(max(self.y1) + 1)
        # print("xmax = ", xmax, "ymax =", ymax)
        # if xmax < 100 or ymax < 300:
        #     return
        # self.lidarmap = [[0 for _ in range(ymax)] for _ in range(xmax)]
        # num_rows = len(self.lidarmap)
        # num_cols = len(self.lidarmap[0])
        # for i in range(len(self.x1)):
        #     row = int(self.x1[i])
        #     col = int(self.y1[i])
        #     for t in range(max(0, row - 1), min(num_rows, row + 10)):
        #         for j in range(max(0, col - 1), min(num_cols, col + 10)):
        #             self.lidarmap[t][j] = 1
        # print(mymap)

    def findRoad(self, start, goal):
        self.path = self.astar(start, goal)
        if self.path:
            print("æ‰¾åˆ°äº†")
            # print("è·¯å¾„ï¼š", self.path)
        else:
            print("æ— æ³•åˆ°è¾¾ç›®æ ‡èŠ‚ç‚¹")

    def displayImg(self, lableIngo):  # å‚æ•°ä¸ºï¼š ç‰©ä½“çš„åæ ‡å’Œæ ‡ç­¾
        plt.figure(figsize=(10.8, 9.6), dpi=100)  # è®¾ç½®ç”»å¸ƒå¤§å° 1080 * 960 çš„ç”»å¸ƒ
        plt.autoscale(enable=False, tight=False)
        plt.xlabel('x')
        plt.ylabel('y')
        # å¯»è·¯çš„åæ ‡ç‚¹ï¼Œä¹‹ååº”è¯¥æ˜¯ä¸€ä¸ªæ ˆæ¥å­˜å‚¨ã€‚
        # start1 = (0, 0)
        # goal1 = (100, 300)
        while True:
            xobj = []
            yobj = []
            obj = []
            lidar_loc = []
            showdatax = []
            showdatay = []
            roadx = []
            roady = []
            while lableIngo.qsize() != 0:
                txt, x, y = lableIngo.get()
                obj.append(txt)
                xobj.append(x)
                yobj.append(y)
                lidar_loc.append(x / 20)  # ä¸€ä»½20pxå®½,å¾—åˆ°ç›®æ ‡å—å· [0 - 32]
            # print("x = ", xobj, "y = ", yobj)
            """
            é‡‡ç”¨ç½‘æ ¼æ³• å°†å›¾åƒæ•°æ®æ˜ å°„åˆ°é›·è¾¾å›¾åƒã€‚
            """
            length = len(xobj)
            # æ±‚å– å¯¹åº”çš„é›·è¾¾åæ ‡
            # 580 -- 850 å¯¹åº”ä¸º 89 -- 108Â° å¯¹åº”px ä¸º 250 -- 440
            # æ‰€ä»¥ è¿™ä¸‰ä¸ªæ¤…å­çš„ä½ç½® å¯¹åº”ä¸ºlidar_loc åœ¨ 12 -- 22 ä¹‹é—´çš„éƒ½ä¸ºæœ‰æ•ˆ
            lidar_loc.sort()  # å¾—åˆ°å¯¹åº”çš„å—å·[0-32]æ’åºã€‚1å¯¹åº”èŒƒå›´ä¸º18 - 22Â°ï¼Œ2å¯¹åº”22Â°-38Â°, 3å¯¹åº” 38Â°-58Â°
            # print(lidar_loc)  # 6 9 18 19 21  50 - 140Â° å¯¹åº” 0 - 640  20Â° = 7*20px  7å€çš„å…³ç³»
            for i in range(length):  # æ ¹æ®ä¸åŒçš„å—å·æ˜¾ç¤ºä¸åŒåŒºåŸŸçš„å›¾åƒï¼Œä»å·¦åˆ°å³
                if i < length - 1:
                    mina = lidar_loc[i] * 20 - 5  # æœ€å°xåæ ‡
                    maxa = lidar_loc[i] * 20 + 5  # æœ€å¤§xåæ ‡
                    # print("èŒƒå›´ä¸º", (mina, maxa))
                    len1 = len(self.lidarAngle_old)
                    print(len1)
                    for j in range(1, len1):
                        if self.lidarDis_old[j] < 50:
                            pass
                            # self.Control.Mykey.dataStop()  # è·ç¦»è¿‡è¿‘ç›´æ¥åœè½¦

                        if ((50 + zcar_jiao + mina) / 8) < (self.lidarAngle_old[j] + zcar_jiao) < (
                                (50 + zcar_jiao + maxa) / 8):  # æ³¨æ„è¦æ‰¾åˆ°å›¾åƒåæ ‡å¯¹åº”çš„é›·è¾¾çš„è§’åº¦ä¿¡æ¯
                            self.lidarAngle_old[j] = self.lidarAngle_old[j] * (np.pi / 180)  # æåæ ‡
                            x11, y11 = self.polar_to_cartesian(self.lidarDis_old[j], self.lidarAngle_old[j])
                            showdatax.append(x11)
                            showdatay.append(y11)

            # åŠ å…¥ä¹è½´ä¼ æ„Ÿå™¨ä½¿å¾—å›¾åƒå¯¹åº”çš„è§’åº¦å§‹ç»ˆä¸ºè½¦è¾†æ­£å‰æ–¹
            valdata = (90 + zcar_jiao) * (np.pi / 180)  # æåæ ‡
            for i in range(50):
                x11, y11 = self.polar_to_cartesian(i * 20, valdata)
                roadx.append(x11)
                roady.append(y11)
            # print(showdatax, showdatay)
            # print(lidar_x, lidar_loc)
            self.parsingLidarData()  # è§£ææ•°æ®
            self.generationMap()  # ç”Ÿæˆåœ°å›¾
            # self.findRoad(start1, goal1)  # æ˜¾ç¤º
            # xx = []
            # yy = []
            # print(len(self.path))
            # for i in range(len(self.path)):
            #     xx.append(self.path[i][0])
            #     yy.append(self.path[i][1])
            plt.clf()  # æ¸…å›¾
            plt.scatter(0, 0, s=40, marker="^")  # åŸç‚¹
            plt.scatter(roadx, roady, s=40, marker="^")  # 90Â°è·¯å¾„
            plt.scatter(self.x1, self.y1, s=1)  # åœ°å›¾
            plt.scatter(showdatax, showdatay, s=10, marker="o")  # éšœç¢ç‰©
            # for i in range(length):
            #     plt.text(showdatax[i], showdatay[i], obj[i])
            # plt.scatter(xx, yy, s=1)  # è·¯å¾„
            plt.xlim((-1300, 1300))
            plt.ylim((-600, 1300))
            plt.title('Cartesian Coordinate System')
            plt.grid(True)
            plt.pause(0.001)  # æš‚åœä¸€æ®µæ—¶é—´ï¼Œä¸ç„¶ç”»çš„å¤ªå¿«ä¼šå¡ä½æ˜¾ç¤ºä¸å‡ºæ¥


@smart_inference_mode()
def run(
        weights=ROOT / 'yolov5n.pt',  # model path or triton URL
        source=ROOT / 'data/images',  # file/dir/URL/glob/screen/0(webcam)
        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path
        imgsz=(640, 640),  # inference size (height, width)
        conf_thres=0.25,  # confidence threshold
        iou_thres=0.45,  # NMS IOU threshold
        max_det=1000,  # maximum detections per image
        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        view_img=False,  # show results
        save_txt=False,  # save results to *.txt
        save_conf=False,  # save confidences in --save-txt labels
        save_crop=False,  # save cropped prediction boxes
        nosave=False,  # do not save images/videos
        classes=None,  # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,  # class-agnostic NMS
        augment=False,  # augmented inference
        visualize=False,  # visualize features
        update=False,  # update all models
        project=ROOT / 'runs/detect',  # save results to project/name
        name='exp',  # save results to project/name
        exist_ok=False,  # existing project/name ok, do not increment
        line_thickness=3,  # bounding box thickness (pixels)
        hide_labels=False,  # hide labels
        hide_conf=False,  # hide confidences
        half=False,  # use FP16 half-precision inference
        dnn=False,  # use OpenCV DNN for ONNX inference
        vid_stride=1,  # video frame-rate stride
):
    source = str(source)
    save_img = not nosave and not source.endswith('.txt')  # save inference images
    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)
    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))
    webcam = source.isnumeric() or source.endswith('.streams') or (is_url and not is_file)
    screenshot = source.lower().startswith('screen')
    if is_url and is_file:
        source = check_file(source)  # download

    # Directories
    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run
    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir

    # Load model
    device = select_device(device)
    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
    stride, names, pt = model.stride, model.names, model.pt
    imgsz = check_img_size(imgsz, s=stride)  # check image size

    # Dataloader
    bs = 1  # batch_size
    if webcam:
        view_img = check_imshow(warn=True)
        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)
        bs = len(dataset)
    elif screenshot:
        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)
    else:
        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)
    vid_path, vid_writer = [None] * bs, [None] * bs

    # Run inference
    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup
    seen, windows, dt = 0, [], (Profile(), Profile(), Profile())
    for path, im, im0s, vid_cap, s in dataset:
        with dt[0]:
            im = torch.from_numpy(im).to(model.device)
            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32
            im /= 255  # 0 - 255 to 0.0 - 1.0
            if len(im.shape) == 3:
                im = im[None]  # expand for batch dim

        # Inference
        with dt[1]:
            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False
            pred = model(im, augment=augment, visualize=visualize)

        # NMS
        with dt[2]:
            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)

        # Second-stage classifier (optional)
        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)

        # Process predictions
        for i, det in enumerate(pred):  # per image
            seen += 1
            if webcam:  # batch_size >= 1
                p, im0, frame = path[i], im0s[i].copy(), dataset.count
                s += f'{i}: '
            else:
                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)

            p = Path(p)  # to Path
            save_path = str(save_dir / p.name)  # im.jpg
            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt
            s += '%gx%g ' % im.shape[2:]  # print string
            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
            imc = im0.copy() if save_crop else im0  # for save_crop

            annotator = Annotator(im0, line_width=line_thickness, example=str(names))
            if len(det):
                # Rescale boxes from img_size to im0 size
                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()

                # Print results
                for c in det[:, 5].unique():
                    n = (det[:, 5] == c).sum()  # detections per class
                    s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string

                # Write results
                for *xyxy, conf, cls in reversed(det):
                    if save_txt:  # Write to file
                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format
                        with open(f'{txt_path}.txt', 'a') as f:
                            f.write(('%g ' * len(line)).rstrip() % line + '\n')

                    if save_img or save_crop or view_img:  # Add bbox to image
                        c = int(cls)  # integer class
                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')
                        annotator.box_label(xyxy, label, color=colors(c, True))
                        if label == "person" or "persons":
                            p1, p2 = (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3]))
                            p_x = (p2[0] - p1[0]) / 2 + p1[0]
                            p_y = (p2[1] - p1[1]) / 2 + p1[1]
                            # print("px = ", p_x)
                            if p_x <= 640:
                                lableIngo.put((label, p_x, p_y))

                        # æ‰“å°åæ ‡ä¿¡æ¯
                        # print("ä¸­å¿ƒç‚¹çš„åæ ‡" + str(Center))
                    if save_crop:
                        save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)

            # Stream results
            im0 = annotator.result()
            if view_img:
                if platform.system() == 'Linux' and p not in windows:
                    windows.append(p)
                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)
                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])

                # print("ç”»é¢æ˜¾ç¤º")

                cv2.imshow("EyeHeaven", im0)
                cv2.waitKey(1)  # 1 millisecond

            # Save results (ismage with detection)
            if save_img:
                if dataset.mode == 'image':
                    cv2.imwrite(save_path, im0)
                else:  # 'video' or 'stream'
                    if vid_path[i] != save_path:  # new video
                        vid_path[i] = save_path
                        if isinstance(vid_writer[i], cv2.VideoWriter):
                            vid_writer[i].release()  # release previous video writer
                        if vid_cap:  # video
                            fps = vid_cap.get(cv2.CAP_PROP_FPS)
                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        else:  # stream
                            fps, w, h = 30, im0.shape[1], im0.shape[0]
                        save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos
                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
                    vid_writer[i].write(im0)

        # Print time (inference-only)
        LOGGER.info(f"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1E3:.1f}ms")

    # Print results
    t = tuple(x.t / seen * 1E3 for x in dt)  # speeds per image
    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)
    if save_txt or save_img:
        s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ''
        LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}{s}")
    if update:
        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)


#  default='runs/train/exp6/weights/best.pt',
# F:/CreatCar/wyt_data/Python/yolo/yolov5/runs/train/exp6/weights/best.pt

def parse_opt():
    parser = argparse.ArgumentParser()
    # parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp6/weights/best.pt',
    #                     help='model path or triton URL')
    parser.add_argument('--weights', nargs='+', type=str, default='yolov5n.pt',
                        help='model path or triton URL')
    parser.add_argument('--source', type=str, default=0, help='source')
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[1280, 640],
                        help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='show results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--visualize', action='store_true', help='visualize features')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')
    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    parser.add_argument('--vid-stride', type=int, default=1, help='video frame-rate stride')
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    print_args(vars(opt))
    return opt


# class YoloAndLidar(ç½‘ç»œè¿œç¨‹æ§åˆ¶éƒ¨åˆ†.ç‰ˆæœ¬3.Modules.è‡ªåŠ¨é¿éšœ.EmergencyStop.EmergencyStop):
#     pass


def main(opt):
    check_requirements(exclude=('tensorboard', 'thop'))
    run(**vars(opt))


def yolo_start():
    opt = parse_opt()
    main(opt)


"""
æ€è·¯ :
 åœ¨æ‘„åƒå¤´å›¾åƒçš„æŸä¸ªèŒƒå›´å†… é›·è¾¾æ˜¯å¯ä»¥è®¡ç®—å‡ºç‰©ä½“çš„è·ç¦»ï¼Œä½†æ˜¯åœ¨å›¾åƒä¸Šæ–¹æˆ–è€…ä¸‹æ–¹ï¼Œæ˜¯æ²¡æ³•å¾—åˆ°å…·ä½“è·ç¦»çš„
 å¯¹äºæ¯ä¸ª2Dè¾¹ç•Œæ¡†ï¼Œå›¾åƒæ£€æµ‹æä¾›ç±»åˆ«ã€‚å¯¹äºæ¯ä¸ªLiDARæŠ•å½±ç‚¹ï¼Œæˆ‘ä»¬æœ‰å‡†ç¡®çš„è·ç¦»ã€‚
 ç»†èŠ‚å¤„ç† 1. æˆ‘ä»¬å¾—åˆ°çš„è¾¹ç•Œæ¡†ä¸€èˆ¬éƒ½ä¼šæ¯”å®é™…ç‰©ä½“å¤§ï¼Œ
        è§£å†³æ–¹æ¡ˆ 1. å¾—åˆ°ä¸­å¿ƒç‚¹å»å‘å‘¨å›´åŠ¨æ€æ‰©æ•£æŒ‡å®šå¤§å°ï¼Œç®€å•ï¼Œä½†æ˜¯å¯¹äºå¤ªå°çš„ç‰©ä½“ä¼šä½¿å…¶èŒƒå›´å˜å¤§ã€‚
        è§£å†³æ–¹æ¡ˆ 2. é‡‡ç”¨å›¾åƒåˆ†å‰²ã€‚
èåˆæ€è·¯ï¼š
    è¯†åˆ«å‡ºä½ç½®ï¼Œä¸”é›·è¾¾å»é‡ç‚¹æµ‹é‡è¿™ä¸ªåœ°æ–¹å‘¨å›´çš„è·ç¦»ï¼Œæœ€åå…¥æ ˆï¼Œ
    
å°†åŒç›®æ‘„åƒå¤´æ•°æ®åœ¨å›¾åƒä¸­æ ‡æ³¨å‡ºæ¥ã€‚åœ°å›¾æ ‡æ³¨
    æ€è·¯ï¼š é›·è¾¾å’Œå›¾åƒåˆ†ä¸ºä¸¤ä¸ªè¿›ç¨‹å»å¤„ç†ï¼Œ åœ¨è¿™è¦åŠ å…¥è¿›ç¨‹åŒæ­¥ã€‚ç¡®ä¿å›¾åƒå’Œé›·è¾¾æ•°æ®æ˜¯ä¸€ç»„ã€‚
    ä¹‹åé€šè¿‡ å¤šè¿›ç¨‹ ç®¡é“ï¼Œ å°†å›¾åƒå¾—åˆ°çš„ ç‰©ä½“ä¿¡æ¯å¤§æ¦‚ä½ç½®ä¼ é€’åˆ° é›·è¾¾è¿›ç¨‹ï¼Œ
    ä¹‹åé›·è¾¾è¿›ç¨‹å»å¤„ç†è¿™ä¸ªä¿¡æ¯ï¼Œä¹‹åæ˜¾ç¤ºåœ¨é›·è¾¾å›¾åƒä¸­ï¼Œåšå‡ºæ ‡è¯†ã€‚
    ä¹‹ååœ¨å¼•å…¥A*å¯»è·¯ã€‚
    
    æµ‹è¯•å¾—åˆ° å›¾åƒåæ ‡åœ¨(270,270),(450,400) èŒƒå›´å†…
    å¤§è‡´å¯¹åº”äºé›·è¾¾æ­£ä¸­çš„ä½ç½®
"""


class DataMerge:
    # æ•°æ®èåˆå±•ç¤º
    def dataMergeDisplay(self):
        t3 = myThread_zitai("å§¿æ€", True)
        t3.start()
        t1 = threading.Thread(target=yolo_start)  # å®ä¾‹åŒ–è¿›ç¨‹å¯¹è±¡
        t1.start()
        p2 = multiprocessing.Process(target=Lidar().displayImg(lableIngo))
        p2.start()


if __name__ == '__main__':
    a = DataMerge()
    a.dataMergeDisplay()
